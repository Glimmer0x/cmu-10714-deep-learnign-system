# Deep Learning System 笔记（统一符号口径 + DRAM/行主序直觉）

> 目标：把数学推导（列向量传统）和工程实现（C/C++/NumPy/PyTorch 常见“样本为行”）统一到同一套 shape 逻辑里；并把损失函数与噪声分布的对应关系、Softmax-CE 的梯度推导串起来。

---

## 0. 符号与形状约定（先把口径固定）

我常用工程口径（batch-first）：

- 单样本特征：$x^{(i)} \in \mathbb{R}^n$ (实现里通常当作长度为 $n$ 的向量)
- 数据矩阵：$X \in \mathbb{R}^{m \times n}$，每行一个样本($m$ 个样本，$n$ 维特征)
- 多分类参数：$\theta \in \mathbb{R}^{n \times k}$($k$ 类)
- logits：
  - 单样本：$h(x^{(i)}) \in \mathbb{R}^k$
  - 批量：$H = X\theta \in \mathbb{R}^{m \times k}$

### 列向量传统写法怎么对应到工程写法？

数学里很多时候把单样本当列向量($n \times 1$)，把样本堆成矩阵：

$$
\tilde X = [x^{(1)}, \dots, x^{(m)}] \in \mathbb{R}^{n \times m}.
$$

这时 logits 常写成：

$$
\theta^\top \tilde X \in \mathbb{R}^{k \times m}.
$$

工程里用 $X = \tilde X^\top$（样本为行），所以：

$$
X\theta = (\tilde X^\top)\theta \quad\Longleftrightarrow\quad (\theta^\top \tilde X)^\top.
$$

结论：两套写法只差转置；关键是整条链路中不要混用“样本按行/按列”两套记号。

---

## 1. 为什么实现里“样本为行”很自然（DRAM / cache / 行主序）

我一直觉得推导里“列向量”更干净，但实现里（C/C++/NumPy/PyTorch）经常“样本为行”更顺手，底层原因和内存访问模式有关。

### 1.1 DRAM：一次激活读整行（Row Buffer）

- DRAM 按“行”组织：wordline 连接一整行存储单元；每个单元有电容（存电荷代表 0/1）+ 晶体管（开关）。
- 读的时候对 wordline 加电压打开晶体管，电容信号被感应放大，**一次会把整行（约 8KB–16KB）读进 row buffer**。
- 之后从 row buffer 再分块搬运到更上层。

### 1.2 CPU cache line：64B 粒度 + 连续访问更快

- cache 典型以 64B cache line 作为搬运单位。
- 如果数组是行主序（row-major）并且我按行连续访问：
  - cache 层面：更容易命中/更少 cache miss（连续 64B）
  - DRAM 层面：更可能复用同一次 row activation 的 row buffer（8–16KB）
- 所以行主序 + 按行访问会更快，这也解释了为什么工程里常把 **batch 放第 0 维**，用 GEMM：$X\theta$。

---

## 2. 损失函数 = 噪声分布假设下的最大似然

把观测写成：

$$
y = f(x;\theta) + \epsilon
$$

选择不同的噪声分布 $\epsilon$，等价于选择不同的负对数似然（loss）。

---

## 3. L2 / MSE：高斯噪声 → 最小二乘；常数预测最优是均值

### 3.1 高斯噪声推到 L2

若 $\epsilon\sim \mathcal{N}(0,\sigma^2)$，则：

$$
\max_\theta \log p(y|x,\theta) \;\Longleftrightarrow\; \min_\theta \sum_i (y_i - f(x_i;\theta))^2
$$

（系数与常数项忽略不影响最优点。）

### 3.2 只预测一个常数 $a$ 时

$$
J(a)=\mathbb{E}[(Y-a)^2]
$$

求导：

$$
\frac{\partial J}{\partial a} = -2\mathbb{E}[Y] + 2a = 0
\Rightarrow a=\mathbb{E}[Y]
$$

偏差-方差分解（好记）：

$$
\mathbb{E}[(Y-a)^2]
= \underbrace{\mathbb{E}\left[(Y-\mathbb{E}[Y])^2\right]}_{\mathrm{Var}(Y)} + \underbrace{(\mathbb{E}[Y]-a)^2}_{\text{Bias}^2}
$$

结论：在 L2 下，最优常数预测是**mean (均值)**。

---

## 4. L1：拉普拉斯噪声 → 绝对值损失；常数预测最优是中位数

若 $\epsilon\sim \text{Laplace}(0,b)$，其密度：

$$
p(\epsilon)=\frac{1}{2b}\exp(-|\epsilon|/b)
$$

负对数似然对应：

$$
\min_a \mathbb{E}[|Y-a|]
$$

最优条件：

$$
P(Y>a)=P(Y<a)
$$

结论：L1 下常数预测最优是 **median（中位数）**。

---

## 5. 鲁棒损失：Huber 与 Student-t 的直觉

### 5.1 Huber loss（折中 L1 与 L2）

- 小残差：二次（像 L2，梯度平滑）
- 大残差：一次（像 L1，对 outlier 不敏感）

### 5.2 Student-t（厚尾）与“混合”的直觉

- Student-t 相比高斯更厚尾，所以对异常点更鲁棒。
- 常见理解：它可以视作“高斯在方差/精度上做随机化后的混合”（混合后尾部变厚）。

---

## 6. Cross Entropy：从 Bernoulli 到 Categorical/Multinomial

### 6.1 二分类：Bernoulli

令 $p=f_\theta(x)$，则：

$$
P(y|x,\theta)=p^y(1-p)^{1-y}
$$

log-likelihood：

$$
\log P(y|x,\theta) = y\log p + (1-y)\log(1-p)
$$

取负号就是 binary cross entropy。

### 6.2 多分类：Categorical Cross Entropy（one-hot）

概率向量 $p=[p_1,\dots,p_k],\sum_j p_j=1$，label one-hot $y$：

$$
\log P(y|x,\theta)=\sum_j y_j\log p_j
$$

负号即 CCE。

### 6.3 Multinomial（n 次试验的计数形式）

$$
P(n_1,\dots,n_k) = \frac{n!}{\prod_i n_i!}\prod_i p_i^{n_i}.
$$

当 $n=1$ 时退化为 categorical（单样本）情形。


---

## 7. Softmax + CE：核心等式与梯度（单样本 → 批量）

### 7.1 Softmax

$$
z_i = \frac{e^{h_i}}{\sum_j e^{h_j}},\qquad \sum_i z_i=1
$$

### 7.2 单样本 CE（label = y）

$$
\ell(h,y) = -\log z_y = -h_y + \log\sum_j e^{h_j}
$$

对 logits 的梯度（完整向量形式）：

$$
\frac{\partial \ell}{\partial h_i} = z_i - \mathbf{1}[i=y]
$$

也就是：

$$
\delta = z - \mathrm{onehot}(y)
$$

### 7.3 批量形式最终梯度（最重要的结论）

设：

- $H=X\theta\in\mathbb{R}^{m\times k}$
- $Z=\mathrm{softmax}(H)\in\mathbb{R}^{m\times k}$（按行 softmax）
- $Y\in\mathbb{R}^{m\times k}$（one-hot）
- $\Delta = Z - Y$

则：

$$
\frac{\partial \mathcal{L}}{\partial \theta} = X^\top (Z-Y) = X^\top \Delta
$$

这个式子非常“工程友好”：实现里 $\Delta$ 就是预测概率减去 one-hot。

---

## 8. 矩阵微分 / trace trick 速查（用来机械推导）

常用三条：

1. differential 与 trace：

$$
df = \mathrm{tr}\!\left(\left(\frac{\partial f}{\partial X}\right)^\top dX\right)
$$

2. 乘积微分：

$$
d(AB) = dA\cdot B + A\cdot dB
$$

3. trace 循环置换：

$$
\mathrm{tr}(ABC)=\mathrm{tr}(BCA)=\mathrm{tr}(CAB)
$$

典型模板（推 $\partial \ell / \partial \theta$)：

- 先写

$$
d\ell = \mathrm{tr}(\Delta^\top dH)
$$

- 再写

$$
dH = d(X\theta) = X\, d\theta
$$

- 合并并循环置换：

$$
\begin{aligned}
d\ell
&= \mathrm{tr}(\Delta^\top X\, d\theta) \\
&= \mathrm{tr}\!\left((X^\top \Delta)^\top d\theta\right)
\end{aligned}
$$

因此：

$$
\frac{\partial \ell}{\partial \theta} = X^\top \Delta
$$

---

## 9. 一页总结

- 推导“列向量” vs 实现“样本为行”不是矛盾：只要 shape 全程一致，本质只差转置。
- 工程偏好 batch-first/样本为行：符合 row-major 布局下的空间局部性，同时更贴合 cache line 与 DRAM row buffer 的访问模式。
- 损失函数选择本质是噪声分布假设：Gaussian→L2（均值），Laplace→L1（中位数），Huber/Student-t→更鲁棒。
- Softmax+CE 的梯度关键中间量：$\Delta=Z-Y$，最终参数梯度：$X^\top\Delta$。
